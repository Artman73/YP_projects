{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Построим модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузим и подготовим данные.\n",
    "2. Обучим разные модели. \n",
    "3. Сделаем выводы.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импорт библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "#хитмэп корреляции\n",
    "import os\n",
    "import seaborn as sns #построение графиков\n",
    "\n",
    "#операции с выборкой\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#сохранение модели\n",
    "import joblib \n",
    "from joblib import dump\n",
    "\n",
    "#классификация\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "#регрессия\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor #создание константной модели (среднее, медиана и др)\n",
    "\n",
    "#преобразователь масштаба методом стандартизации\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#метрики\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score #кросс-валидация\n",
    "from sklearn.metrics import make_scorer #для написания ручной оценки\n",
    "\n",
    "#подбор параметров\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#построение ROC-кривой\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#раздел upsampling\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#библиотека LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "#библиотека CatBoost\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#разложение временного ряда на тренд и сезонную компоненту, построение графиков этих составляющих ряда\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#замена стандартной кросс-валидации для временных рядов\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "#для графиков\n",
    "import pylab\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "#оценка тональности текстов\n",
    "import torch # %pip install torchvision\n",
    "import transformers as ppb # %pip install transformers\n",
    "from tqdm import notebook\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загрузим датасет\n",
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')   \n",
    "except FileNotFoundError:\n",
    "    data = pd.read_csv('toxic_comments.csv')\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bert1 = data.sample(100, random_state=12345).reset_index(drop=True)\n",
    "data_bert2 = data.sample(3000, random_state=12345).reset_index(drop=True)\n",
    "\n",
    "#shorted_512 = data['lemm_text'].apply(lambda x: re.sub(r'^(.{512}).*$', '\\g<1>', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "    Данные загружены, 159570 строк, столбец text содержит текст комментариев на английском, а toxic — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pre-trained DistilBERT model and tokenizer\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "#Importing pre-trained BERT model and tokenizer:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "#Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = data_bert1['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(np.array(padded))\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the output for the first position for all the sequences, take all hidden unit outputs\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_bert1['toxic']\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the Logistic Regression model on the training set.\n",
    "lr_clf = LogisticRegression(random_state=12345, max_iter=1000)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the model is trained, we can score it against the test set\n",
    "predictions_train = lr_clf.predict(train_features) \n",
    "predictions_test = lr_clf.predict(test_features)\n",
    "\n",
    "f1_train = f1_score(train_labels, predictions_train)\n",
    "f1_test = f1_score(test_labels, predictions_test)\n",
    "\n",
    "print(f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "#tokenizer = transformers.BertTokenizer(vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "tokenized = data_bert2['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = transformers.BertConfig.from_json_file('/datasets/ds_bert/bert_config.json')\n",
    "#model = transformers.BertModel.from_pretrained('/datasets/ds_bert/rubert_model.bin', config=config)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccc4dad10b64d5392b918554fab1baf",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучим и протестируем модель\n",
    "target = data_bert2['toxic']\n",
    "features = np.concatenate(embeddings)\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(target_train.shape, target_test.shape, features_train.shape, features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=12345, max_iter=10000)\n",
    "model.fit(features_train, target_train) \n",
    "\n",
    "predictions_train = model.predict(features_train) \n",
    "predictions_test = model.predict(features_test)\n",
    "\n",
    "f1_train = f1_score(target_train, predictions_train)\n",
    "f1_test = f1_score(target_test, predictions_test)\n",
    "print(f1_train, f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод\n",
    "    На версии BERT 2 достигнуто целевое значение метрики f1 0,75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод\n",
    "    Первая версия взята из источника http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    При первых запусках показывала f1 около 0,25, при следующих запусках - 0.0.\n",
    "    \n",
    "    С помощью второй версии из теории получили метрику f1 = 0.77 согласно цели проекта. \n",
    "    BERT ресурсоемкая модель, поэтому получилось запустить (при 12Gb ОЗУ, если выбираем больше 3000 - то ошибка нехватки памяти) проект и достить нужного значения метрики - благодаря размеру sample 3000 в сочетании с test_size=0.1. При большем размере sample, метрика f1 вероятно будет выше. Начинал с sample 500, повышая каждый раз на 500 размер выборки, прирост f1 с каждым увеличением выборки - 3-5%.\n",
    "    \n",
    "    Для второй версии несяно где брать 2 параметра: \n",
    "    #config = transformers.BertConfig.from_json_file('/datasets/ds_bert/bert_config.json')\n",
    "    и\n",
    "    #model = transformers.BertModel.from_pretrained('/datasets/ds_bert/rubert_model.bin', config=config)\n",
    "    Эти 2 параметра взяты из первого кода BERT, неясно насколько это корректно.\n",
    "    \n",
    "    Для повышения результатов метрики возможно проверить и устранить дисбаланс классов и повысить количество samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 62078,
    "start_time": "2022-06-25T16:11:55.064Z"
   },
   {
    "duration": 2451,
    "start_time": "2022-06-25T16:13:20.686Z"
   },
   {
    "duration": 854,
    "start_time": "2022-06-25T16:13:26.158Z"
   },
   {
    "duration": 876,
    "start_time": "2022-06-25T16:13:33.014Z"
   },
   {
    "duration": 624,
    "start_time": "2022-06-25T16:15:27.754Z"
   },
   {
    "duration": 59880,
    "start_time": "2022-06-25T16:21:09.159Z"
   },
   {
    "duration": 2460,
    "start_time": "2022-06-25T16:22:09.042Z"
   },
   {
    "duration": 13,
    "start_time": "2022-06-25T16:22:11.504Z"
   },
   {
    "duration": 1074,
    "start_time": "2022-06-25T16:22:11.520Z"
   },
   {
    "duration": 19131,
    "start_time": "2022-06-25T16:22:12.597Z"
   },
   {
    "duration": 576,
    "start_time": "2022-06-25T16:22:31.730Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-25T16:22:32.309Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-25T16:22:32.311Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-25T16:22:32.313Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-25T16:22:32.315Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-25T16:22:32.317Z"
   },
   {
    "duration": 49520,
    "start_time": "2022-06-26T14:53:05.271Z"
   },
   {
    "duration": 2299,
    "start_time": "2022-06-26T14:53:54.793Z"
   },
   {
    "duration": 7,
    "start_time": "2022-06-26T14:53:57.093Z"
   },
   {
    "duration": 1193,
    "start_time": "2022-06-26T14:53:57.102Z"
   },
   {
    "duration": 18707,
    "start_time": "2022-06-26T14:53:58.296Z"
   },
   {
    "duration": 51440,
    "start_time": "2022-06-26T14:55:53.819Z"
   },
   {
    "duration": 2360,
    "start_time": "2022-06-26T14:56:45.261Z"
   },
   {
    "duration": 7,
    "start_time": "2022-06-26T14:56:47.623Z"
   },
   {
    "duration": 1049,
    "start_time": "2022-06-26T14:56:47.632Z"
   },
   {
    "duration": 18668,
    "start_time": "2022-06-26T14:56:48.683Z"
   },
   {
    "duration": 49762,
    "start_time": "2022-06-26T14:58:26.052Z"
   },
   {
    "duration": 2325,
    "start_time": "2022-06-26T14:59:15.815Z"
   },
   {
    "duration": 7,
    "start_time": "2022-06-26T14:59:18.141Z"
   },
   {
    "duration": 573,
    "start_time": "2022-06-26T14:59:18.150Z"
   },
   {
    "duration": 18663,
    "start_time": "2022-06-26T14:59:18.725Z"
   },
   {
    "duration": 47,
    "start_time": "2022-06-26T15:00:01.374Z"
   },
   {
    "duration": 56133,
    "start_time": "2022-06-26T15:00:16.144Z"
   },
   {
    "duration": 2303,
    "start_time": "2022-06-26T15:01:12.280Z"
   },
   {
    "duration": 6,
    "start_time": "2022-06-26T15:01:14.585Z"
   },
   {
    "duration": 602,
    "start_time": "2022-06-26T15:01:14.593Z"
   },
   {
    "duration": 18741,
    "start_time": "2022-06-26T15:01:15.196Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
